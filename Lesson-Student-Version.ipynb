{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical power "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing, Type I and Type II error rates\n",
    "\n",
    "When we perform a hypothesis test, we may reject or fail to reject the null hypothesis about a population parameter. In a hypothesis test, the null hypothesis states that there is no difference between our groups, whereas the alternative hypothesis states that there is a difference between the groups we're testing. We reject a null hypothesis if the p-value we obtain is less than $\\alpha$, the significance level of our test. \n",
    "\n",
    "$\\alpha$ is the probability of rejecting a null hypothesis when it's actually true.\n",
    "\n",
    "There are four possible outcomes when performing statistical hypothesis tests: \n",
    "\n",
    "<img src=\"images/confusion_matrix.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "Imagine you are running a clinical trial for a new drug, and you want to test whether patient symptoms improve more rapidly after treatment with the drug than after a placebo treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the null hypothesis in this case?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the Type I and Type II errors in this context?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Type I Error \n",
    "\n",
    "If you reject a null hypothesis that is actually correct, you are making a type I error.\n",
    "\n",
    "* $\\alpha$, the significance level of a hypothesis test, is the probability of making a type I error. \n",
    "\n",
    "<img src=\"images/rejected.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Type I Error Simulation\n",
    "\n",
    "Imagine we have two samples of scores from the same population of scores. This population is normally distributed with mean 10 and standard deviation 1.\n",
    "\n",
    "Although we know that the two samples are from the same population of scores, there is still a small probability of seeing a large difference between mean sample values and of incorrectly rejecting the null hypothesis that the samples come from the same population, that is, of making a type I error. \n",
    "\n",
    "**Let's run 1000 two-sample t-tests to compute type 1 error rate.** \n",
    "\n",
    "We will repeatedly (`n_simulations = 1000` times):\n",
    "* take two independent samples from the same population, \n",
    "* compute a two-sided t-test with significance level $\\alpha = 0.05$, and \n",
    "* keep count of the number of times we reject the null hypothesis, even though we know it to be true \n",
    "\n",
    "The goal is to compute type I error rate.\n",
    "\n",
    "Remember, type I error is when you reject the null hypothesis given it is true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we run our simulations, **what are the null and alternative hypotheses for the tests we're performing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Inspect the code below, which computes type I error rate, before running it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1000 tests performed, the null hypothesis was incorrectly rejected 51 times.\n",
      "\n",
      "The type I error rate is 0.051.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import scipy.stats as stats\n",
    "import numpy as np \n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create an instance of a normal continuous random variable with mean 10 and standard deviation 1\n",
    "scores_population = stats.norm(loc=10, scale=1)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = scores_population.rvs(n_sample_size), scores_population.rvs(n_sample_size)\n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    if result[1] < alpha:\n",
    "        c+=1\n",
    "\n",
    "type_1_error_rate = c/n_simulations\n",
    "\n",
    "print(\"Out of {} tests performed, the null hypothesis was incorrectly rejected {} times.\".format(n_simulations, c))\n",
    "print(\"\\nThe type I error rate is {}.\".format(type_1_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Type II Error \n",
    "\n",
    "If we fail to reject a null hypothesis that is actually false, you are making a type II error. \n",
    "\n",
    "* We use $\\beta$ to denote the probability of making a type II error.\n",
    "\n",
    "<img src=\"images/false.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Type II Error Simulation\n",
    "\n",
    "Now, imagine that we have two samples of scores from two different populations that are normally distributed, one with mean 5 and standard deviation 1, and the other with mean 6 and standard deviation 2. \n",
    "\n",
    "Even though we know that the two samples are from different populations, there is still the chance that we will think the samples come from the same population even though we know they do not - that is, there is a chance we will fail to reject the null hypothesis. In this case, we would be committing a Type II error. \n",
    "\n",
    "**Let's run some simulations to compute type 2 error rate in this scenario.** \n",
    "\n",
    "You will repeatedly (`n_simulations = 1000`):\n",
    "* take two samples, with sample size equal to 25, from the two different populations, \n",
    "* compute a two-sided t-test with significance level $\\alpha = 0.05$, and \n",
    "* keep count of the number of times we fail to reject the null hypothesis, even though we know it to be false. \n",
    "\n",
    "The goal is to compute type II error rate. Remember, type II error is when you fail to reject the null hypothesis given it is false.\n",
    "\n",
    "**What are the null and alternative hypotheses in this case?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Fill out the missing code below to run the simulation to compute type II error rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instances of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = None\n",
    "scores_population_2 = None\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw from each population to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis if the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you fail to reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    \n",
    "    sample1, sample2 = None, None\n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of whether the null hypothesis was rejected or not to update the counter\n",
    "    None  #HERE IS WHERE YOU NEED TO ADD CODE\n",
    "\n",
    "type_2_error_rate = None\n",
    "\n",
    "print(\"Out of {} tests performed, the null hypothesis was not rejected {} times.\".format(n_simulations, c))\n",
    "print(\"\\nThe type II error rate is {}.\".format(type_2_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happens if you change the sample size to 10 instead of 25?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Power "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we perform a statistical hypothesis test, we want to make sure that if the null hypothesis isn't true, we're able to reject the null hypothesis. We want to be able to detect a difference between the groups if there is one.\n",
    "\n",
    "Statistical power is the probability that we will reject the null hypothesis given it is actually false. Thus, $\\text{power} = 1 - \\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The statistical power of a hypothesis test is a function of:\n",
    "* the sample size, \n",
    "* the significance level $\\alpha$, and \n",
    "* the effect size or difference between the groups we are testing\n",
    "\n",
    "Typically accepted values for the power of a statistical test are greater than or equal to 0.80 or 80%. Studies with power less than 80% are said to be underpowered and require a reevaluation of experimental design or acquiring more samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Effect size \n",
    "\n",
    "When we design an experiment, we want to make sure to gather enough data to be able to detect differences between our groups, should the difference exist. The effect size is a measure of the difference between the two groups we're testing. Effect sizes are easy to calculate, understand and apply to any measured outcome and is applicable to a multitude of study domains. It is highly valuable towards quantifying the effectiveness of a particular intervention, relative to some comparison. Measuring effect size allows scientists to go beyond the obvious and simplistic, 'Does it work or not?' to the far more sophisticated, 'How well does it work in a range of contexts?'\n",
    "\n",
    "<img src=\"images/doesitmatter.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohen's d, denoted by $d$, is a _standardized_ effect size measure equal to the magnitude of the difference in sample means divided by the pooled sample standard deviation of the two samples. \n",
    "* We use standardized effect sizes so we can remove the units of the variables in the effect size.  \n",
    "\n",
    "When testing the difference in the sample means of two samples, we use Cohen's d to measure the effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cohen's d\n",
    "\n",
    "Cohen's d is given by: \n",
    "\n",
    "$$ \\large d = \\frac{|\\mu_2 - \\mu_1|}{s_p},  $$\n",
    "\n",
    "where $\\mu_1$ and $\\mu_2$ are the sample means for sample 1 and 2, respectively, and $s_p$ is the pooled standard deviation of the two samples. \n",
    "\n",
    "The pooled standard deviation $s_p$ of the two samples is given by: \n",
    "\n",
    "$$ \\large s_p = \\sqrt{\\frac{\\left(n_1 -1\\right)s_1^2 + \\left(n_2 -1\\right)s_2^2 }{n_1 + n_2 - 2}}, $$\n",
    "\n",
    "where $n_1$ and $n_2$ are the sample sizes for sample 1 and sample 2, respectively, and $s_1^2$ and $s_2^2$ are the sample variances for sample 1 and sample 2, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cohen_d(sample1, sample2):\n",
    "    n1, n2 = len(sample1), len(sample2)\n",
    "    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)\n",
    "    pooled_var = ((n1-1)*var1 + (n2-1)*var2)/(n1+n2-2)\n",
    "    s = np.sqrt(pooled_var)\n",
    "    \n",
    "    mean1, mean2 = np.mean(sample1), np.mean(sample2)\n",
    "    \n",
    "    return np.abs(mean2-mean1)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Effect sizes are considered to be small, medium, or large depending on the following rule of thumb: \n",
    "\n",
    "||Cohen's d|\n",
    "|--|--|\n",
    "|small|0.2|\n",
    "|medium|0.5|\n",
    "|large|0.8|\n",
    "\n",
    "[Check this out](https://rpsychologist.com/d3/cohend/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compute the effect size for the following two samples, `sample1` and `sample2`, using the function for Cohen's d written above. Is this a small, medium, or large effect size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.078156931213827"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42) #for reproducibility\n",
    "rv1 = stats.norm(loc=10, scale=1)\n",
    "rv2 = stats.norm(loc=12, scale=2)\n",
    "\n",
    "sample1 = rv1.rvs(25)\n",
    "sample2 = rv2.rvs(25)\n",
    "\n",
    "cohen_d(sample1, sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What if the sample size was 50?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Why did the effect size increase when we increased the sample size of our samples?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- - - \n",
    "\n",
    "Now that we have seen effect size in more detail, let's go back to our example to compute type II error rate using a simulation and let's compute power instead.\n",
    "\n",
    "In this example, remember that the null hypothesis is that the two samples of scores came from the same population of scores, and the alternative hypothesis is that they do not come from the same population of scores. \n",
    "\n",
    "**When you compute the power of a statistical test, what probability are you calculating?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Fill out the missing code to run 1000 simulations and compute the power of our test.** \n",
    "\n",
    "(Remember that $\\text{power} = 1 - \\beta$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instance of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = None\n",
    "scores_population_2 = None\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = None, None # START FILLING IN CODE HERE\n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of the number of times you reject the null hypothesis \n",
    "    pass \n",
    "\n",
    "type_2_error_rate = None\n",
    "\n",
    "power = None \n",
    "\n",
    "print(\"Power: {}\".format(power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**If we were limited to a sample size of 10, what would be the power of our test?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happened to the power of the test as we changed the sample size?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Power and effect size, sample size, and $\\alpha$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following four quantities are interrelated:\n",
    "* power\n",
    "* effect size\n",
    "* sample size\n",
    "* significance level, $\\alpha$ \n",
    "\n",
    "Given any of these three quantities, we can determine the fourth. \n",
    "\n",
    "Let's explore how power depends on effect size, sample size, and significance level $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function below allows you to compute power for any `effect_size`, `sample_size` and `alpha` combination using `n_simulations` simulated tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_power(effect_size, sample_size, alpha, n_simulations=1000):\n",
    "    \n",
    "    rv1 = stats.norm(loc=0, scale=1)\n",
    "    rv2 = stats.norm(loc=effect_size, scale=1)\n",
    "    \n",
    "    # keep a count of the times you failed to reject the null hypothesis\n",
    "    c = 0\n",
    "    for i in range(n_simulations):\n",
    "        sample1, sample2 = rv1.rvs(sample_size), rv2.rvs(sample_size)\n",
    "        result = stats.ttest_ind(sample1, sample2)\n",
    "        if result[1] > alpha:\n",
    "            c+=1\n",
    "            \n",
    "    beta = c/n_simulations\n",
    "    power = 1 - beta\n",
    "    \n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens to the power of a two-sided t-test as the sample size is changed?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Create a plot to show how power changes as sample size changes.** \n",
    "\n",
    "Use `sample_sizes = [10, 20, 50, 100]`\n",
    "\n",
    "Assume $\\alpha=0.05$ and that you want to measure an effect size equal to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happens to the power of a two-sided t-test as the effect size we want to detect changes? Create a plot to show how power changes as the effect size changes.** \n",
    "\n",
    "Use `effect_sizes = [0.1, 0.2, 0.5, 0.8]`. \n",
    "\n",
    "Assume alpha=0.05 and sample_size=100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What happens to the power of a statistical test as $\\alpha$ changes? Create a plot to show how power changes as $\\alpha$ changes.** \n",
    "\n",
    "Use `alphas = [0.001, 0.01, 0.05, 0.1, 0.2]`. \n",
    "\n",
    "Assume sample_size=100 and that you're trying to measure an effect_size = 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How is all of this useful?: Power analysis for experimental design\n",
    "\n",
    "You've seen that the power of a statistical test increases as sample size is increased, the effect size is increased, or the significance level $\\alpha$ is increased. \n",
    "\n",
    "When designing an experiment, before gathering samples, we should determine the sample size we need if we want our test to be powerful enough to detect differences between groups. \n",
    "* If we know the desired significance level, the desired power of our test, and the magnitude of the effect size we're trying to measure, we can determine the sample size we need to have to detect a difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case Study\n",
    "\n",
    "A researcher wants to compare two different diets, diets A and B, in diabetic mice. The researcher hypothesizes that diet A will be better than diet B, in terms of lower blood glucose. She gets a random sample of 30 diabetic mice, and randomly assigns them to one of the two diets. At the end of the experiment, which lasts 10 weeks, a blood glucose test will be conducted on each mouse. She expects the average difference in blood glucose measurement between the two groups of mice to be about 10 mg/dl. Based on past results, a common standard deviation of 15 mg/dl will be used for each treatment group in the power analysis. \n",
    "\n",
    "Is this a practical experimental design? Perform a power analysis simulation. Assume $\\alpha = 0.05$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is the required sample size to identify a difference of 5 mg/dl between the groups with 80% power?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower, TTestPower\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the smallest difference in blood glucose levels that the researcher can currently detect given her current sample size, if she wants the power of her test to be 80% and she wants to present results at $\\alpha = 0.05$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Another Case Study: \n",
    "\n",
    "A teacher wants to study how daily tutoring sessions for students will affect students' overall test grades. \n",
    "\n",
    "The study will allow the enrollment of 30 students. Half will be randomized to a control group and not undergo any tutoring; the other half will receive daily tutoring sessions at the end of the class day. The tutoring will be carried out over 30 days. \n",
    "\n",
    "The teacher wants to know whether mean student grades after 30 days differ between the two groups in the study, those who were tutored and those who were not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What's the null hypothesis in this case? What's the alternative hypothesis?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The teacher wants to know what power will be obtained under the sample size restrictions to identify a change in the mean grade of 5 points. Based on past results, a common standard deviation of 10 will be used for each treatment group in the power analysis. \n",
    "\n",
    "Perform a power analysis simulation to determine if this is a practical experimental design. Use $\\alpha = 0.05$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What sample size would we need to detect a 5 point difference in test scores with a power of 80% and $\\alpha = 0.05$?**\n",
    "\n",
    "Hint: What effect size does this difference correspond to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
